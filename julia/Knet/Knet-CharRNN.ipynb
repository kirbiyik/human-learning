{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"4934845-element Array{UInt8,1}\", \"526731-element Array{UInt8,1}\", \"84-element Array{Char,1}\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet\n",
    "include(Knet.dir(\"data\",\"gutenberg.jl\"))\n",
    "trn,tst,chars = shakespeare()\n",
    "# summary(x) returns a string with a brief description. \n",
    "# By default returns string(typeof(x)), e.g. Int64.\n",
    "# mapping applies summary to other arguments to give a\n",
    "# brief look to variables, doesn't change anything.\n",
    "map(summary,(trn,tst,chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Char,1}:\n",
       " 'a'\n",
       " 'n'\n",
       " 'd'\n",
       " ' '\n",
       " 't'\n",
       " 'i'\n",
       " 'm'\n",
       " 'e'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 84 unique characters in the data and they are mapped to UInt8 values in 1:84.\n",
    "# The chars array can be used to recover the original text:\n",
    "chars[trn][end-97: end - 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCHSIZE = 256\n",
    "SEQLENGTH = 100\n",
    "\n",
    "function minibatch_rnn(data)\n",
    "    num_batch = div(length(data), BATCHSIZE)\n",
    "    # reshape full data to (B,num_batch) with contiguous rows\n",
    "    # \n",
    "    #x = reshape(data[1:num_batch * BATCHSIZE], num_batch, BATCHSIZE)'\n",
    "    x = reshape(data[1:num_batch * BATCHSIZE], BATCHSIZE, num_batch)\n",
    "    # split into (B,T) blocks\n",
    "    # Meaning of : x[:,1:num_batch-1], x[:,2:num_batch]\n",
    "    # Remember in char-rnn we were feeding next char as, sort of,\n",
    "    # label to the data. We sample at t, then compare with t+1. Then\n",
    "    # obtain loss.\n",
    "    minibatch(x[:,1:num_batch-1], x[:,2:num_batch], SEQLENGTH) \n",
    "end\n",
    "\n",
    "dtrain, dtest = minibatch_rnn(trn), minibatch_rnn(tst)\n",
    "map(length, (dtrain, dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNTYPE = :lstm  # can be :lstm, :gru, :tanh, :relu\n",
    "NUMLAYERS = 1    # number of RNN layers\n",
    "INPUTSIZE = 168  # size of the input character embedding\n",
    "HIDDENSIZE = 334 # size of the hidden layers\n",
    "VOCABSIZE = 84   # number of unique characters in data\n",
    "\n",
    "# To run on GPU, use KnetArray\n",
    "# d... means varargs, 0 or more arguments\n",
    "# xavier directly passes varargs to rand in implementation\n",
    "function initmodel()\n",
    "    w(d...)=xavier(Float32,d...)\n",
    "    b(d...)=zeros(Float32,d...)\n",
    "    # r is rnn struct.\n",
    "    # w: single weight array that includes all matrices and biases for the RNN\n",
    "    r,wr = rnninit(INPUTSIZE,HIDDENSIZE,rnnType=RNNTYPE,numLayers=NUMLAYERS)\n",
    "    # input embedding matrix\n",
    "    wx = w(INPUTSIZE,VOCABSIZE)\n",
    "    # wy: hidden state to output\n",
    "    wy = w(VOCABSIZE,HIDDENSIZE)\n",
    "    by = b(VOCABSIZE,1)\n",
    "    return r,wr,wx,wy,by\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not fully understood\n",
    "function predict(ws, xs, hx, cx)\n",
    "    # ws expected to have this structure\n",
    "    r,wr,wx,wy,by = ws\n",
    "    # TODO digest line below\n",
    "    x = wx[:,xs] # xs=(Batch,Time) x=(X,B,T)\n",
    "    # y=(H,B,T) hy=cy=(H,B,L)\n",
    "    y,hy,cy = rnnforw(r, wr, x, hx, cx, hy=true, cy=true)\n",
    "    ys = by.+wy*reshape(y,size(y,1),size(y,2)*size(y,3)) # ys=(H,B*T)\n",
    "    return ys, hy, cy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(w, x, y, h)\n",
    "    # h[1]-> hx, h[2]-> cx\n",
    "    py, hy, cy = predict(w, x, h...)\n",
    "    # In order AutoGrad to work we need getval somehow..\n",
    "    h[1], h[2] = getval(hy), getval(cy)\n",
    "    return nll(py, y)\n",
    "end\n",
    "\n",
    "# reports loss and calculates grads\n",
    "lossgradient = gradloss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model, data, optim)\n",
    "    # rnn forwards assumes zero vector when hidden is nothing\n",
    "    hiddens = Any[nothing, nothing]\n",
    "    losses = []\n",
    "    for (x, y) in data\n",
    "        grads, current_loss = lossgradient(model, x, y, hiddens)\n",
    "        update!(model, grads, optim)\n",
    "        push!(losses, current_loss)\n",
    "    end\n",
    "    return mean(losses)\n",
    "end\n",
    "\n",
    "function test(model, data)\n",
    "    hiddens = Any[nothing, nothing]\n",
    "    losses = []\n",
    "    for (x, y) in data\n",
    "        current_loss = loss(model, x, y, hiddens)\n",
    "        push!(losses, current_loss)\n",
    "    end\n",
    "    return mean(losses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983.938376 seconds (49.50 M allocations: 490.977 GiB, 21.88% gc time)\n",
      " 38.377965 seconds (2.58 M allocations: 12.148 GiB, 7.83% gc time)\n",
      "(:epoch, 1, :trnppl, 25.758282f0, :tstppl, 23.529913f0)\n",
      "891.528233 seconds (44.52 M allocations: 490.720 GiB, 22.46% gc time)\n",
      " 34.552269 seconds (945.09 k allocations: 12.077 GiB, 8.61% gc time)\n",
      "(:epoch, 2, :trnppl, 24.428978f0, :tstppl, 23.530243f0)\n",
      "1108.173357 seconds (44.52 M allocations: 490.720 GiB, 37.96% gc time)\n",
      " 35.052935 seconds (945.09 k allocations: 12.077 GiB, 8.78% gc time)\n",
      "(:epoch, 3, :trnppl, 24.41434f0, :tstppl, 23.518257f0)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mvector_any\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Any, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./essentials.jl:331\u001b[22m\u001b[22m",
      " [2] \u001b[1m(::AutoGrad.##rfun#7#10{AutoGrad.#broadcast#tanh})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::AutoGrad.Rec{Array{Float32,2}}, ::Vararg{AutoGrad.Rec{Array{Float32,2}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/core.jl:133\u001b[22m\u001b[22m",
      " [3] \u001b[1mbroadcast#tanh\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Float32,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [4] \u001b[1mtanh\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/unfuse.jl:45\u001b[22m\u001b[22m [inlined]",
      " [5] \u001b[1m(::Knet.##507#535)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Broadcasted{AutoGrad.Rec{Array{Float32,2}}}, ::AutoGrad.Broadcasted{AutoGrad.Rec{Array{Float32,2}}}, ::AutoGrad.Broadcasted{AutoGrad.Rec{Array{Float32,1}}}, ::AutoGrad.Broadcasted{AutoGrad.Rec{Array{Float32,1}}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [6] \u001b[1mbroadcast\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::AutoGrad.Rec{Array{Float32,2}}, ::AutoGrad.Rec{Array{Float32,2}}, ::AutoGrad.Rec{Array{Float32,1}}, ::Vararg{AutoGrad.Rec{Array{Float32,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/unfuse.jl:35\u001b[22m\u001b[22m",
      " [7] \u001b[1m#rnntest#498\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Void, ::Bool, ::Bool, ::Array{Any,1}, ::Function, ::Knet.RNN, ::AutoGrad.Rec{Array{Float32,3}}, ::AutoGrad.Rec{Array{Float32,3}}, ::Array{Float32,3}, ::Array{Float32,3}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/Knet/src/rnn.jl:768\u001b[22m\u001b[22m",
      " [8] \u001b[1m(::Knet.#kw##rnntest)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Knet.#rnntest, ::Knet.RNN, ::AutoGrad.Rec{Array{Float32,3}}, ::AutoGrad.Rec{Array{Float32,3}}, ::Array{Float32,3}, ::Array{Float32,3}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [9] \u001b[1m#rnnforw#497\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Knet.RNN, ::AutoGrad.Rec{Array{Float32,3}}, ::AutoGrad.Rec{Array{Float32,3}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/Knet/src/rnn.jl:657\u001b[22m\u001b[22m",
      " [10] \u001b[1m(::Knet.#kw##rnnforw)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Knet.#rnnforw, ::Knet.RNN, ::AutoGrad.Rec{Array{Float32,3}}, ::AutoGrad.Rec{Array{Float32,3}}, ::Array{Float32,3}, ::Array{Float32,3}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [11] \u001b[1m#rnnforw#488\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::AutoGrad.Rec{Knet.RNN}, ::AutoGrad.Rec{Array{Float32,3}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/Knet/src/rnn.jl:547\u001b[22m\u001b[22m",
      " [12] \u001b[1m(::Knet.#kw##rnnforw)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Knet.#rnnforw, ::AutoGrad.Rec{Knet.RNN}, ::AutoGrad.Rec{Array{Float32,3}}, ::AutoGrad.Rec{Array{Float32,3}}, ::Array{Float32,3}, ::Array{Float32,3}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [13] \u001b[1mpredict\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}}}, ::Array{UInt8,2}, ::Array{Float32,3}, ::Array{Float32,3}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[5]:8\u001b[22m\u001b[22m",
      " [14] \u001b[1mloss\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}}}, ::Array{UInt8,2}, ::Array{UInt8,2}, ::Array{Any,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[6]:3\u001b[22m\u001b[22m",
      " [15] \u001b[1mforward_pass\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Tuple{Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}},Array{UInt8,2},Array{UInt8,2},Array{Any,1}}, ::Array{Any,1}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/core.jl:88\u001b[22m\u001b[22m",
      " [16] \u001b[1m(::AutoGrad.##gradfun#4#6{#loss,Int64})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/core.jl:57\u001b[22m\u001b[22m",
      " [17] \u001b[1m(::AutoGrad.#gradfun#5)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/computa/.julia/v0.6/AutoGrad/src/core.jl:57\u001b[22m\u001b[22m",
      " [18] \u001b[1mtrain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Knet.RNN,Array{Float32,3},Array{Float32,2},Array{Float32,2},Array{Float32,2}}, ::Knet.MB, ::Tuple{Void,Knet.Adam,Knet.Adam,Knet.Adam,Knet.Adam}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[7]:6\u001b[22m\u001b[22m",
      " [19] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [20] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[8]:6\u001b[22m\u001b[22m [inlined]",
      " [21] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [22] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "# Let's train\n",
    "EPOCHS = 10\n",
    "model = initmodel()\n",
    "optim = optimizers(model, Adam)\n",
    "@time for epoch in 1:EPOCHS\n",
    "    @time trnloss = train(model,dtrain,optim) # ~18 seconds\n",
    "    @time tstloss = test(model,dtest)        # ~0.5 seconds\n",
    "    println((:epoch, epoch, :trnppl, exp(trnloss), :tstppl, exp(tstloss)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(model, n)\n",
    "    function sample(y)\n",
    "        p, r = Array(exp.(y - logsumexp(y))), rand()\n",
    "        for j=1:length(p)\n",
    "            (r -= p[j]) < 0 && return j\n",
    "        end\n",
    "    end\n",
    "    h,c = nothing,nothing\n",
    "    x = findfirst(chars,'\\n')\n",
    "    for i=1:n\n",
    "        y,h,c = predict(model,[x],h,c)\n",
    "        x = sample(y)\n",
    "        print(chars[x])\n",
    "    end\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I've trained for a very short time, it's no surpise that it did not converged.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tl'U4donsenooro cah,oien\n",
      " s.A  ,ina ih tprwt m  eeh  iLr gs  l o?d\n",
      "resw  itetlhaeOyo\n",
      "t hd\n",
      " e   ret,lr  a  en\n",
      "IuepuoaetOo enRpovhh it  s bictcEHdn.e\n",
      "  mnot  hi\n",
      "uyb  rhs   Rredott.svR at\n",
      " shO ldN Uaogb saatone W s,o   hhpsors  dTdia  t oo \n",
      "I.wYn  attnmoei h  wt eaTwO Tnn.  g s \n",
      "nYbibtirtsr narljs A\n",
      "orloekoa a\n",
      " R\n",
      "Ciosioe c ntbc i  moeeoU  a  d HTIf, t\n",
      "sot, aO,loe y ! dAod  hOgas nhe lopAyeeafewra eedteD oos  m\n",
      "dt w  t.IBe w acefatona   t  e set yh o s wuotae!sy t oh dtep\n",
      "rsvcnh uorA  Iiaen teab d tTf h H  tg y Ed.rsnm' a l ns\n",
      "hcM: ie f i loT  lywet  tefDhli c efM h  d li;    d\n",
      " si\n",
      "ini; ch,w\n",
      "n AiTn mhV tn do obs\n",
      "urRua knAye \n",
      "ct wwairh TL q .hsAwFpA t IooHpsNr Seg xh nt nngshuho  t  soUo  aHbs e gnh sg as'.isrTKdn\n",
      "Wi  biEnhooneA fSossmcoo  d e,oInoeii tAEml.orOd i , \n",
      "Adra lT  E.m ss  Se'he w\n"
     ]
    }
   ],
   "source": [
    "generate(model,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
